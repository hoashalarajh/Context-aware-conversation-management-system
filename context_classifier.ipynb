{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hoashalarajh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# importing spacy library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a list of stop words\n",
    "import string\n",
    "punct = string.punctuation\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stopwords = list(STOP_WORDS) # list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function for data cleaning\n",
    "def text_data_cleaning(sentence):\n",
    "  doc = nlp(sentence)\n",
    "\n",
    "  tokens = [] # list of tokens\n",
    "  for token in doc:\n",
    "    if token.lemma_ != \"-PRON-\":\n",
    "      temp = token.lemma_.lower().strip()\n",
    "    else:\n",
    "      temp = token.lower_\n",
    "    tokens.append(temp)\n",
    " \n",
    "  cleaned_tokens = []\n",
    "  for token in tokens:\n",
    "    if token not in stopwords and token not in punct:    # Stopwords and punctuation removal\n",
    "      cleaned_tokens.append(token)\n",
    "  return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hoashalarajh\\AppData\\Local\\Temp\\ipykernel_10096\\4244303213.py:5: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  model = pickle.load(f)\n",
      "c:\\Users\\Hoashalarajh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.2.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Hoashalarajh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.2.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Hoashalarajh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.2.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Hoashalarajh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 1.2.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Hoashalarajh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator Pipeline from version 1.2.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "import pickle\n",
    "# load the saved model from file\n",
    "with open('Context_selector.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with user examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['suggestions']\n"
     ]
    }
   ],
   "source": [
    "sent = [\"give me some eastern food suggestions\"]\n",
    "print (model.predict(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['suggestions']\n"
     ]
    }
   ],
   "source": [
    "sent = [\"Can you add some sci-fi novels ?\"]\n",
    "print (model.predict(sent))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Bir-gram Model to Predict the suitable response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using ngrams - SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Crichton wrote Andromeda Strain\n"
     ]
    }
   ],
   "source": [
    "# Using N-grams (Bi-gram) Model to predict the suitble resonse\n",
    "import spacy\n",
    "\n",
    "# Load the spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define the list of possible responses\n",
    "response_list = [\"Michael Crichton wrote Jurassic Park\", \"Lee Child wrote Killing Floor\", \"Michael Crichton wrote Andromeda Strain\"]\n",
    "\n",
    "# Define the user input\n",
    "user_input = \"Who wrote Andromeda Strain?\"\n",
    "\n",
    "# Convert the user input to a spacy Doc object\n",
    "doc = nlp(user_input)\n",
    "\n",
    "# Define the number of n-grams to generate\n",
    "n = 2\n",
    "\n",
    "# Generate the n-grams from the user input\n",
    "user_ngrams = []\n",
    "for i in range(len(doc)-n+1):\n",
    "    ngram = doc[i:i+n]\n",
    "    user_ngrams.append(' '.join(str(token) for token in ngram))\n",
    "\n",
    "# Loop through the possible responses and find the one with the highest n-gram overlap\n",
    "max_overlap = 0\n",
    "best_response = \"\"\n",
    "for response in response_list:\n",
    "    # Convert the response to a spacy Doc object\n",
    "    response_doc = nlp(response)\n",
    "    \n",
    "    # Generate the n-grams from the response\n",
    "    response_ngrams = []\n",
    "    for i in range(len(response_doc)-n+1):\n",
    "        ngram = response_doc[i:i+n]\n",
    "        response_ngrams.append(' '.join(str(token) for token in ngram))\n",
    "    \n",
    "    # Calculate the n-gram overlap\n",
    "    overlap = len(set(user_ngrams) & set(response_ngrams))\n",
    "    \n",
    "    # Update the best response if the current response has a higher overlap\n",
    "    if overlap > max_overlap:\n",
    "        max_overlap = overlap\n",
    "        best_response = response\n",
    "\n",
    "# Print the best response\n",
    "print(best_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael Crichton', 'Crichton wrote', 'wrote Andromeda', 'Andromeda Strain']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Who wrote', 'wrote Andromeda', 'Andromeda Strain', 'Strain ?']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_overlap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using ngrams - NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Crichton wrote Andromeda Strain\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Define the list of possible responses\n",
    "response_list = [\"Michael Crichton wrote Jurassic Park\", \"Lee Child wrote Killing Floor\", \"Michael Crichton wrote Andromeda Strain\"]\n",
    "\n",
    "# Define the user input\n",
    "user_input = \"Who wrote Andromeda Strain ?\"\n",
    "\n",
    "# Tokenize the user input\n",
    "user_tokens = word_tokenize(user_input)\n",
    "\n",
    "# Define the number of n-grams to generate\n",
    "n = 2\n",
    "\n",
    "# Generate the n-grams from the user input\n",
    "user_ngrams = list(ngrams(user_tokens, n))\n",
    "\n",
    "# Loop through the possible responses and find the one with the highest n-gram overlap\n",
    "max_overlap = 0\n",
    "best_response = \"\"\n",
    "for response in response_list:\n",
    "    # Tokenize the response\n",
    "    response_tokens = word_tokenize(response)\n",
    "    \n",
    "    # Generate the n-grams from the response\n",
    "    response_ngrams = list(ngrams(response_tokens, n))\n",
    "    \n",
    "    # Calculate the n-gram overlap\n",
    "    overlap = len(set(user_ngrams) & set(response_ngrams))\n",
    "    \n",
    "    # Update the best response if the current response has a higher overlap\n",
    "    if overlap > max_overlap:\n",
    "        max_overlap = overlap\n",
    "        best_response = response\n",
    "\n",
    "# Print the best response\n",
    "print(best_response)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Bi-gram Model for Probability Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency table:\n",
      "Counter({('The', 'quick'): 1, ('quick', 'brown'): 1, ('brown', 'fox'): 1, ('fox', 'jumps'): 1, ('jumps', 'over'): 1, ('over', 'the'): 1, ('the', 'lazy'): 1, ('lazy', 'dog'): 1})\n",
      "Probability table:\n",
      "{('The', 'quick'): 0.125, ('quick', 'brown'): 0.125, ('brown', 'fox'): 0.125, ('fox', 'jumps'): 0.125, ('jumps', 'over'): 0.125, ('over', 'the'): 0.125, ('the', 'lazy'): 0.125, ('lazy', 'dog'): 0.125}\n"
     ]
    }
   ],
   "source": [
    "# probability estimation using N-grams Model\n",
    "import collections\n",
    "\n",
    "# Define the input text\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = text.split()\n",
    "\n",
    "# Define the n-gram size\n",
    "n = 2\n",
    "\n",
    "# Generate the n-grams\n",
    "ngrams = []\n",
    "for i in range(len(tokens)-n+1):\n",
    "    ngram = tuple(tokens[i:i+n])\n",
    "    ngrams.append(ngram)\n",
    "\n",
    "# Count the frequency of each n-gram\n",
    "freq_table = collections.Counter(ngrams)\n",
    "\n",
    "# Calculate the total number of n-grams\n",
    "total_ngrams = len(ngrams)\n",
    "\n",
    "# Estimate the probability of each n-gram\n",
    "prob_table = {}\n",
    "for ngram, freq in freq_table.items():\n",
    "    prob = freq / total_ngrams\n",
    "    prob_table[ngram] = prob\n",
    "\n",
    "# Print the frequency and probability table for the n-grams\n",
    "print(\"Frequency table:\")\n",
    "print(freq_table)\n",
    "print(\"Probability table:\")\n",
    "print(prob_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
